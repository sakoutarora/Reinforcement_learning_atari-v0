{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc9d166a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8edec454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create env \n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02878bb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0118315 ,  0.01325219,  0.02903985, -0.03485435], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()  # takes game to the initial state also return the intital state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed61ed26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game Episode : 0/20 High Score 19\n",
      "Game Episode : 1/20 High Score 12\n",
      "Game Episode : 2/20 High Score 12\n",
      "Game Episode : 3/20 High Score 23\n",
      "Game Episode : 4/20 High Score 25\n",
      "Game Episode : 5/20 High Score 12\n",
      "Game Episode : 6/20 High Score 31\n",
      "Game Episode : 7/20 High Score 17\n",
      "Game Episode : 8/20 High Score 11\n",
      "Game Episode : 9/20 High Score 48\n",
      "Game Episode : 10/20 High Score 18\n",
      "Game Episode : 11/20 High Score 41\n",
      "Game Episode : 12/20 High Score 8\n",
      "Game Episode : 13/20 High Score 40\n",
      "Game Episode : 14/20 High Score 9\n",
      "Game Episode : 15/20 High Score 22\n",
      "Game Episode : 16/20 High Score 12\n",
      "Game Episode : 17/20 High Score 31\n",
      "Game Episode : 18/20 High Score 31\n",
      "Game Episode : 19/20 High Score 12\n",
      "All epi over\n"
     ]
    }
   ],
   "source": [
    "for e in range(20): \n",
    "    obser = env.reset()\n",
    "    for t in range(100):\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        state,reward,done,_ = env.step(action)\n",
    "        if done : \n",
    "            print('Game Episode : {}/{} High Score {}'.format(e,20,t))\n",
    "            break\n",
    "        \n",
    "\n",
    "env.close()\n",
    "print('All epi over')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76fcf9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ed4f511",
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent: \n",
    "    def __init__(self, _state_size, _action_size):\n",
    "        self.state_size = _state_size\n",
    "        self.action_size = _action_size   \n",
    "        \n",
    "        self.gamma = 0.95   # penalise future reward \n",
    "        self.epsilon = 1    #exploration to exploitation ratio at start we wanr the algorithm to be random  \n",
    "        self.epsilon_min = 0.01\n",
    "        \n",
    "        self.epsilon_decay = 0.995  # with greater number of epochs the environment will use more and more of exploitation \n",
    "        self.memory = deque(maxlen = 2000)\n",
    "        self.learning_rate = 1e-3\n",
    "        \n",
    "        self.model = self.create_model()\n",
    "        \n",
    "    def create_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24,input_dim = self.state_size, activation = 'relu'))\n",
    "        model.add(Dense(24,activation = 'relu'))\n",
    "        model.add(Dense(self.action_size, activation = 'linear'))\n",
    "        model.compile(loss = 'mse', optimizer = Adam(learning_rate = self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def buffer(self,state,action,reward,next_state,done):\n",
    "        self.memory.append((state,action,reward,next_state,done))\n",
    "    def act(self,state):\n",
    "        # act done acc to greedy Epsilon method \n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(state)[0])\n",
    "    def train(self,batch_size = 32):\n",
    "        minibatch = random.sample(self.memory,batch_size)\n",
    "        for expe in minibatch:\n",
    "            state,action,reward,next_state,done = expe\n",
    "            if not done: \n",
    "                target = reward + self.gamma*np.amax(self.model.predict(next_state)[0])\n",
    "            else:\n",
    "                target = reward \n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            \n",
    "            self.model.fit(state,target_f,epochs = 1, verbose = 0 )\n",
    "        if self.epsilon > self.epsilon_min :\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    def load(self,name):\n",
    "        self.model.load_weights(name)\n",
    "    def save(self,name):\n",
    "        self.model.save_weights(name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45b22972",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epi = 500\n",
    "state_size = 4 \n",
    "action_size = 2 \n",
    "ag = agent(_state_size=state_size,_action_size=action_size)\n",
    "done = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a763854",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'atari/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11898b86",
   "metadata": {},
   "source": [
    "First was trained for n_epi episodes then for another 70 episodes usins epsilon greedy policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43d43557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game Episode : 0/500 High Score 168 Ecploration rate 0.06380744126877576 \n",
      "Game Episode : 1/500 High Score 199 Ecploration rate 0.06348840406243188 \n",
      "Game Episode : 2/500 High Score 180 Ecploration rate 0.06317096204211972 \n",
      "Game Episode : 3/500 High Score 175 Ecploration rate 0.06285510723190912 \n",
      "Game Episode : 4/500 High Score 139 Ecploration rate 0.06254083169574957 \n",
      "Game Episode : 5/500 High Score 132 Ecploration rate 0.062228127537270826 \n",
      "Game Episode : 6/500 High Score 181 Ecploration rate 0.06191698689958447 \n",
      "Game Episode : 7/500 High Score 173 Ecploration rate 0.061607401965086545 \n",
      "Game Episode : 8/500 High Score 140 Ecploration rate 0.06129936495526111 \n",
      "Game Episode : 9/500 High Score 176 Ecploration rate 0.0609928681304848 \n",
      "Game Episode : 10/500 High Score 199 Ecploration rate 0.060687903789832374 \n",
      "Game Episode : 11/500 High Score 199 Ecploration rate 0.06038446427088321 \n",
      "Game Episode : 12/500 High Score 199 Ecploration rate 0.06008254194952879 \n",
      "Game Episode : 13/500 High Score 199 Ecploration rate 0.05978212923978115 \n",
      "Game Episode : 14/500 High Score 199 Ecploration rate 0.05948321859358224 \n",
      "Game Episode : 15/500 High Score 199 Ecploration rate 0.05918580250061433 \n",
      "Game Episode : 16/500 High Score 199 Ecploration rate 0.058889873488111255 \n",
      "Game Episode : 17/500 High Score 199 Ecploration rate 0.058595424120670696 \n",
      "Game Episode : 18/500 High Score 199 Ecploration rate 0.05830244700006734 \n",
      "Game Episode : 19/500 High Score 199 Ecploration rate 0.058010934765067 \n",
      "Deep Q-learned modeled trained\n"
     ]
    }
   ],
   "source": [
    "for e in range(20):\n",
    "    state = env.reset()\n",
    "    state = state.reshape((1,state_size))\n",
    "    for t in range(500):\n",
    "        env.render()\n",
    "        ac = ag.act(state)\n",
    "        next_state,reward,done,_ = env.step(ac)\n",
    "        reward = reward if not done else -10\n",
    "        next_state = next_state.reshape((1,state_size))\n",
    "        ag.buffer(state,ac,reward,next_state,done)\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            print('Game Episode : {}/{} High Score {} Ecploration rate {} '.format(e,500,t,ag.epsilon))\n",
    "            break\n",
    "        batch_size = 32\n",
    "    if len(ag.memory) > batch_size:\n",
    "        ag.train(batch_size)\n",
    "    if (e%5) == 0:\n",
    "        continue\n",
    "        ag.save(output_file + 'weigts_' + str(e) + '.hdf5')\n",
    "        \n",
    "print('Deep Q-learned modeled trained')\n",
    "env.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d4ae025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44667449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db4ac518",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "state = state.reshape((1,4))\n",
    "for t in range(199):\n",
    "    env.render()\n",
    "    ac = ag.act(state)\n",
    "    state,reward,done,_ = env.step(ac)\n",
    "    state = state.reshape((1,4))\n",
    "    if done:\n",
    "        \n",
    "        print('Failed at {} '. format(t))\n",
    "        break  \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec36365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85d68b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfffd117",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
